{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cvd_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_pretrained_bert as Bert\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import import_ipynb\n",
    "from cvd_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, segment, age\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(config.seg_vocab_size, config.hidden_size)\n",
    "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size)\n",
    "        self.gender_embeddings = nn.Embedding(config.gender_vocab_size, config.hidden_size)\n",
    "        self.ethnicity_embeddings = nn.Embedding(config.ethnicity_vocab_size, config.hidden_size)\n",
    "        self.race_embeddings = nn.Embedding(config.race_vocab_size, config.hidden_size)\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, age_ids=None, gender_ids=None, ethni_ids=None, race_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
    "\n",
    "        if seg_ids is None:\n",
    "            seg_ids = torch.zeros_like(word_ids)\n",
    "        if age_ids is None:\n",
    "            age_ids = torch.zeros_like(word_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "        word_embed = self.word_embeddings(word_ids)\n",
    "        segment_embed = self.segment_embeddings(seg_ids)\n",
    "        age_embed = self.age_embeddings(age_ids)\n",
    "        gender_embed = self.gender_embeddings(gender_ids)\n",
    "        ethnicity_embed = self.ethnicity_embeddings(ethni_ids)\n",
    "        race_embed = self.race_embeddings(race_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "\n",
    "        if age:\n",
    "            embeddings = word_embed + segment_embed + age_embed + gender_embed + ethnicity_embed + race_embed + posi_embeddings\n",
    "        else:\n",
    "            embeddings = word_embed + segment_embed + gender_embed + ethnicity_embed + race_embed +  posi_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config=config)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "    def forward(self, input_ids, age_ids=None, gender_ids=None, ethni_ids=None, race_ids=None, seg_ids=None, posi_ids=None, attention_mask=None,\n",
    "                output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if age_ids is None:\n",
    "            age_ids = torch.zeros_like(input_ids)\n",
    "        if gender_ids is None:\n",
    "            gender_ids = torch.zeros_like(input_ids)\n",
    "        if ethni_ids is None:\n",
    "            ethni_ids = torch.zeros_like(input_ids)\n",
    "        if race_ids is None:\n",
    "            race_ids = torch.zeros_like(input_ids)\n",
    "        if seg_ids is None:\n",
    "            seg_ids = torch.zeros_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, age_ids, gender_ids, ethni_ids, race_ids, seg_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListModule(object):\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "\n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i < 0 or i >= self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMTR(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMTR, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropoutMTR = nn.Dropout(0.3)\n",
    "        self.final = nn.Linear(config.hidden_size, config.number_output)\n",
    "        self.fc_shared = ListModule(self, \"fc1_\")\n",
    "        self.fc_shared1 = ListModule(self, \"fc2_\")\n",
    "        self.fc_shared2 = ListModule(self, \"fc3_\")\n",
    "        for _ in range(5):\n",
    "            self.fc_shared.append(nn.Linear(config.hidden_size, 100))\n",
    "            self.fc_shared1.append(nn.Linear(100, 100))\n",
    "            self.fc_shared2.append(nn.Linear(100, 100))\n",
    "        self.target1 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target2 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target3 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target4 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target5 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target6 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target7 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target8 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target9 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target10 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        self.target11 = nn.Sequential(\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 100),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, 1)\n",
    "        )\n",
    "        \n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def maxout(self, x, layer_list):\n",
    "        max_output = layer_list[0](x)\n",
    "        for _, layer in enumerate(layer_list, start=1):\n",
    "            max_output = torch.max(max_output, layer(x))\n",
    "        return max_output\n",
    "        \n",
    "    def forward(self, input_ids, age_ids=None, gender_ids=None, ethni_ids=None, race_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, masked_lm_labels=None, target_mask=None):\n",
    "        _, pooled_output = self.bert(input_ids, age_ids, gender_ids, ethni_ids, race_ids, seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        prediction_scores = self.dropout(pooled_output)\n",
    "\n",
    "        prediction_scores = self.maxout(prediction_scores, self.fc_shared)\n",
    "        prediction_scores = self.leaky(prediction_scores)\n",
    "        prediction_scores = self.dropoutMTR(prediction_scores)\n",
    "        prediction_scores = self.maxout(prediction_scores, self.fc_shared1)\n",
    "        prediction_scores = self.leaky(prediction_scores)\n",
    "        prediction_scores = self.dropoutMTR(prediction_scores)\n",
    "        prediction_scores = self.maxout(prediction_scores, self.fc_shared2)\n",
    "        prediction_scores = self.leaky(prediction_scores)\n",
    "        prediction_scores = self.dropoutMTR(prediction_scores)\n",
    "\n",
    "        target1 = self.target1(prediction_scores)\n",
    "        target2 = self.target2(prediction_scores)\n",
    "        target3 = self.target3(prediction_scores)\n",
    "        target4 = self.target4(prediction_scores)\n",
    "        target5 = self.target5(prediction_scores)\n",
    "        target6 = self.target6(prediction_scores)\n",
    "        target7 = self.target7(prediction_scores)\n",
    "        target8 = self.target8(prediction_scores)\n",
    "        target9 = self.target9(prediction_scores)\n",
    "        target10 = self.target10(prediction_scores)\n",
    "        target11 = self.target11(prediction_scores)\n",
    "\n",
    "\n",
    "        if masked_lm_labels is not None:\n",
    "            prediction_scores = torch.cat((target1, target2, target3, target4, target5, target6, target7, target8, target9, target10, target11), dim=1)\n",
    "            loss_fct = nn.MSELoss(reduction='none')\n",
    "            masked_lm_loss = loss_fct(prediction_scores, masked_lm_labels)\n",
    "            masked_lm_loss = torch.mul(masked_lm_loss, target_mask)\n",
    "            masked_lm_loss = torch.mean((masked_lm_loss).sum(dim=0)/(target_mask.sum(dim=0) + 0.001))\n",
    "            return masked_lm_loss, prediction_scores, masked_lm_labels\n",
    " \n",
    "        else:\n",
    "            return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.gender_vocab_size = config.get('gender_vocab_size')\n",
    "        self.ethnicity_vocab_size = config.get('ethnicity_vocab_size')\n",
    "        self.race_vocab_size = config.get('race_vocab_size')\n",
    "        self.number_output = config.get('number_output')\n",
    "        self.number_static = config.get('number_static')\n",
    "        \n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVDLoader(Dataset):\n",
    "    def __init__(self, dataframe, max_len, code='code', age='age', labels='labels'):\n",
    "        self.max_len = max_len\n",
    "        self.code = dataframe[code]\n",
    "        self.age = dataframe[age]\n",
    "        self.labels = dataframe[labels]\n",
    "        self.gender = dataframe[\"gender\"]\n",
    "        self.ethnicity = dataframe[\"ethnicity\"]\n",
    "        self.race = dataframe[\"race\"]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        return: age, code, position, segmentation, mask, label\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract data\n",
    "        age = self.age[index]\n",
    "        code = self.code[index]\n",
    "        label = self.labels[index]\n",
    "        gender = self.gender[index]\n",
    "        ethnicity = self.ethnicity[index]\n",
    "        race = self.race[index]\n",
    "\n",
    "        # mask 0:len(code) to 1, padding to be 0\n",
    "        mask = np.ones(self.max_len)\n",
    "        mask[len(code):] = 0\n",
    "        \n",
    "        target_mask = (label != 0).astype(int)\n",
    "\n",
    "        # pad age sequence and code sequence\n",
    "        age = seq_padding(age, self.max_len)\n",
    "        gender = seq_padding(gender, self.max_len)\n",
    "        ethnicity = seq_padding(ethnicity, self.max_len)\n",
    "        race = seq_padding(race, self.max_len)\n",
    "        \n",
    "        # get position code and segment code\n",
    "        code = seq_padding(code, self.max_len)\n",
    "        position = position_idx(code)\n",
    "        segment = index_seg(code)\n",
    "\n",
    "        return  torch.LongTensor(age), torch.LongTensor(code), torch.LongTensor(gender), torch.LongTensor(ethnicity), torch.LongTensor(race), \\\n",
    "                torch.LongTensor(position), torch.LongTensor(segment), \\\n",
    "                torch.FloatTensor(mask), torch.FloatTensor(label), torch.FloatTensor(target_mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(params, config=None):\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'lr': 3e-5,\n",
    "            'warmup_proportion': 0.1,\n",
    "            'weight_decay': 0.01\n",
    "        }\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0}\n",
    "    ]\n",
    "\n",
    "    optim = Bert.optimization.BertAdam(optimizer_grouped_parameters,\n",
    "                                       lr=config['lr'],\n",
    "                                       warmup=config['warmup_proportion'])\n",
    "    return optim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}